{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  <font color='Green'>Forecasting the trend of NIFTY50</font> \n",
    " # <font color='Green'>-----------------------------------------------</font> \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='red'>Introduction</font> \n",
    "\n",
    "<b>NIFTY50 futures is the most traded index in Indian stock markets.<br></br>There are over 2 crore active traders in india and just 10% of them are successfull. <br></br>The success of a day/swing trader depends on their prediction of the current day's trend.\n",
    "\n",
    "The objective of this case study is to predict the current day's trend(just after the start of indian stock markets).\n",
    "<br></br>So by making use of The model's predictions , Day/Swing Traders can plan their trade for any current day.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>ML Formulation</font> \n",
    "<b>Trend on any day can be either Bullish (when closing price of the day is greater than opening price)\n",
    "\n",
    "                                        or\n",
    "\n",
    "<b>Bearish (when closing price is less than opening price). So This is a Binary Classification problem</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Performance Metrics</font> \n",
    "\n",
    "<b> 1. Accuracy(As data is balanced)<br></br>2. F1-Score(As the cost of false positives and false negatives is very high)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Data Sources</font> \n",
    "\n",
    "<b> Data is extracted from the following sources<br></br>\n",
    "\n",
    "        1.NSE(National stock exchange of India),by using nsepy library.\n",
    "        2.Yahoo finance by using yfinance library. \n",
    "        3.Data is also scrapped from way2wealth.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='red'>Data and Feature Extraction</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nsepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date,timedelta\n",
    "from nsepy import get_history\n",
    "from nsepy.derivatives import get_expiry_date\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Futures_Data(Index,from_year,to_year):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function gets the NIFTY50 Futures historical day wise data by making use of the nsepy library. \n",
    "    From the NIFTY50 historical data ,features such as open,high,low,close,volumes are formed.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_months = (to_year - from_year+1)*12\n",
    "    start_date   =  date(from_year,1,1)\n",
    "    expiry_year  =  start_date.year\n",
    "    expiry_month = start_date.month\n",
    "    futures_expiry_date = get_expiry_date(year=expiry_year, month=expiry_month) \n",
    "    futures_expiry_date = list(futures_expiry_date)\n",
    "    futures_expiry_date.sort()\n",
    "    futures_expiry_date = futures_expiry_date[-1]\n",
    "    end_date = futures_expiry_date\n",
    "    Nifty50_data =  get_history(symbol=Index,\n",
    "                    start= start_date,\n",
    "                    end  = end_date,\n",
    "                    index=True,\n",
    "                    futures=True,\n",
    "                    expiry_date=futures_expiry_date)\n",
    "    Nifty50_data = Nifty50_data[['Expiry','Open','High','Low','Close','Number of Contracts','Turnover','Open Interest','Change in OI']]\n",
    "    \n",
    "    for i in tqdm(range(total_months-1)):\n",
    "        prev_expiry_year =  end_date.year\n",
    "        prev_expiry_month = end_date.month\n",
    "        prev_expiry_day = end_date.day\n",
    "        \n",
    "        start_date   =  end_date + timedelta(days=1)\n",
    "        \n",
    "        expiry_year  =  start_date.year\n",
    "        expiry_month =  prev_expiry_month+1\n",
    "        \n",
    "        if(prev_expiry_month == 12):\n",
    "            expiry_month = 1\n",
    "            if(prev_expiry_day!= 31):\n",
    "                expiry_year += 1\n",
    "\n",
    "        futures_expiry_date = get_expiry_date(year=expiry_year, month=expiry_month) \n",
    "        futures_expiry_date = list(futures_expiry_date)\n",
    "        futures_expiry_date.sort()\n",
    "        futures_expiry_date = futures_expiry_date[-1]\n",
    "        end_date = futures_expiry_date\n",
    "\n",
    "        curr_month_Nifty50_data =  get_history(symbol=Index,\n",
    "                                   start= start_date,\n",
    "                                   end  = end_date,\n",
    "                                   index=True,\n",
    "                                   futures=True,\n",
    "                                   expiry_date=futures_expiry_date)\n",
    "        curr_month_Nifty50_data = curr_month_Nifty50_data[['Expiry','Open','High','Low','Close','Number of Contracts','Turnover','Open Interest','Change in OI']]\n",
    "        Nifty50_data=Nifty50_data.append(curr_month_Nifty50_data)\n",
    "        \n",
    "    return Nifty50_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 203/203 [00:40<00:00,  5.03it/s]\n"
     ]
    }
   ],
   "source": [
    "Nifty50_data_df = Get_Futures_Data('Nifty',2004,2020)\n",
    "Nifty50_data_df.to_csv('Nifty50_Futures_Data_2004_to_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candle_stick_pattern_recognizer(data):\n",
    "    \"\"\"\n",
    "    This function takes the Open,High,Low and Closing prices of the data(i.e returned by the previous function)\n",
    "    as input and by making using of talib library,famous candle stick patterns are recognized.\n",
    "    These patterns(each pattern is a feature) are joined with dataframe returned by the previous function.\n",
    "    Features Dataframe, till this stage is returned by this function\n",
    "    \"\"\"\n",
    "    cols_drop = ['Expiry','Number of Contracts','Turnover','Open Interest','Change in OI']\n",
    "    data = data.drop(columns=cols_drop)\n",
    "    \n",
    "    candle_patterns = talib.get_function_groups()['Pattern Recognition']\n",
    "    for pattern in candle_patterns:\n",
    "        data[pattern] = getattr(talib, pattern)(data['Open'],data['High'],data['Low'],data['Close'])\n",
    "    \n",
    "    print(\"TA-LIB can identify a total of {0} candle stick patterns\".format(len(candle_patterns)))\n",
    "    \n",
    "    deleted_patterns = 0\n",
    "    for pattern in candle_patterns:\n",
    "        recognized_pattern_flag = data[pattern].value_counts()\n",
    "        if (recognized_pattern_flag[0] == len(data)):\n",
    "            #print(pattern)\n",
    "            deleted_patterns += 1\n",
    "            data = data.drop(columns=[pattern])\n",
    "        else:\n",
    "            data[pattern] = data[pattern].apply(lambda pat_val: (pat_val/200))\n",
    "   \n",
    "    print(\"In the given data, {0} patterns are identified\".format(len(candle_patterns)-deleted_patterns))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty_data = pd.read_csv('Nifty50_Futures_Data_2004_to_2020.csv')\n",
    "nifty_data = nifty_data.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TA-LIB can identify a total of 61 candle stick patterns\n",
      "In the given data, 51 patterns are identified\n"
     ]
    }
   ],
   "source": [
    "nifty_data_with_candle_stick_patterns = candle_stick_pattern_recognizer(nifty_data.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nifty_data_with_candle_stick_patterns = nifty_data_with_candle_stick_patterns.drop(columns=['Open','High','Low','Close'])\n",
    "nifty_data = nifty_data.drop(columns=['Expiry'])\n",
    "nifty50_with_candle_patterns = pd.concat([nifty_data,nifty_data_with_candle_stick_patterns],axis=1)\n",
    "nifty50_with_candle_patterns.to_csv('Nifty50_With_Candle_Patterns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of Contracts</th>\n",
       "      <th>Turnover</th>\n",
       "      <th>Open Interest</th>\n",
       "      <th>Change in OI</th>\n",
       "      <th>CDL2CROWS</th>\n",
       "      <th>CDL3INSIDE</th>\n",
       "      <th>...</th>\n",
       "      <th>CDLSPINNINGTOP</th>\n",
       "      <th>CDLSTALLEDPATTERN</th>\n",
       "      <th>CDLSTICKSANDWICH</th>\n",
       "      <th>CDLTAKURI</th>\n",
       "      <th>CDLTASUKIGAP</th>\n",
       "      <th>CDLTHRUSTING</th>\n",
       "      <th>CDLTRISTAR</th>\n",
       "      <th>CDLUNIQUE3RIVER</th>\n",
       "      <th>CDLUPSIDEGAP2CROWS</th>\n",
       "      <th>CDLXSIDEGAP3METHODS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-01-01</th>\n",
       "      <td>1894.0</td>\n",
       "      <td>1928.00</td>\n",
       "      <td>1890.1</td>\n",
       "      <td>1925.30</td>\n",
       "      <td>74471</td>\n",
       "      <td>2.851300e+10</td>\n",
       "      <td>7422600</td>\n",
       "      <td>128600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-02</th>\n",
       "      <td>1929.5</td>\n",
       "      <td>1961.00</td>\n",
       "      <td>1929.5</td>\n",
       "      <td>1950.80</td>\n",
       "      <td>73078</td>\n",
       "      <td>2.837095e+10</td>\n",
       "      <td>7770000</td>\n",
       "      <td>347400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-05</th>\n",
       "      <td>1962.8</td>\n",
       "      <td>1969.75</td>\n",
       "      <td>1931.0</td>\n",
       "      <td>1957.15</td>\n",
       "      <td>101796</td>\n",
       "      <td>3.972874e+10</td>\n",
       "      <td>8339000</td>\n",
       "      <td>569000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open     High     Low    Close  Number of Contracts  \\\n",
       "Date                                                                \n",
       "2004-01-01  1894.0  1928.00  1890.1  1925.30                74471   \n",
       "2004-01-02  1929.5  1961.00  1929.5  1950.80                73078   \n",
       "2004-01-05  1962.8  1969.75  1931.0  1957.15               101796   \n",
       "\n",
       "                Turnover  Open Interest  Change in OI  CDL2CROWS  CDL3INSIDE  \\\n",
       "Date                                                                           \n",
       "2004-01-01  2.851300e+10        7422600        128600        0.0         0.0   \n",
       "2004-01-02  2.837095e+10        7770000        347400        0.0         0.0   \n",
       "2004-01-05  3.972874e+10        8339000        569000        0.0         0.0   \n",
       "\n",
       "            ...  CDLSPINNINGTOP  CDLSTALLEDPATTERN  CDLSTICKSANDWICH  \\\n",
       "Date        ...                                                        \n",
       "2004-01-01  ...             0.0                0.0               0.0   \n",
       "2004-01-02  ...             0.0                0.0               0.0   \n",
       "2004-01-05  ...             0.0                0.0               0.0   \n",
       "\n",
       "            CDLTAKURI  CDLTASUKIGAP  CDLTHRUSTING  CDLTRISTAR  \\\n",
       "Date                                                            \n",
       "2004-01-01        0.0           0.0           0.0         0.0   \n",
       "2004-01-02        0.0           0.0           0.0         0.0   \n",
       "2004-01-05        0.0           0.0           0.0         0.0   \n",
       "\n",
       "            CDLUNIQUE3RIVER  CDLUPSIDEGAP2CROWS  CDLXSIDEGAP3METHODS  \n",
       "Date                                                                  \n",
       "2004-01-01              0.0                 0.0                  0.0  \n",
       "2004-01-02              0.0                 0.0                  0.0  \n",
       "2004-01-05              0.0                 0.0                  0.0  \n",
       "\n",
       "[3 rows x 59 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nifty50_with_candle_patterns.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import calendar\n",
    "from talib import SMA,EMA,BBANDS,MOM,MACD,RSI,ADX,MFI,OBV,ADOSC\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(data):\n",
    "    \"\"\"\n",
    "    This function takes the Open,High,Low and Closing prices of the data(returned by the previous function)\n",
    "    as input and by making using of talib librarytechnical indicators such as EMA,Bollinger bands,RSI \n",
    "    MFI and many more are calculated. All these are latter joined in a data frame to form feature matrix.\n",
    "    Features Dataframe, till this stage is returned by this function.\n",
    "    \"\"\"\n",
    "    cols_2_drop = (list(data.columns))\n",
    "    \n",
    "    # EMA and Bollinger Bands\n",
    "    #........................\n",
    "    data['EMA_3']  = EMA(data['Close'],3)\n",
    "    data['EMA_5']  = EMA(data['Close'],5)\n",
    "    data['EMA_8']  = EMA(data['Close'],8)\n",
    "    data['EMA_13'] = EMA(data['Close'],13)\n",
    "    data['EMA_21'] = EMA(data['Close'],21)\n",
    "    data['EMA_34'] = EMA(data['Close'],34)\n",
    "    data['EMA_55'] = EMA(data['Close'],55)\n",
    "    data['EMA_100'] = EMA(data['Close'],100)\n",
    "    data['EMA_200'] = EMA(data['Close'],200)\n",
    "    data['BollUp'], data['BollMid'], data['BollDown'] = BBANDS(data['Close'], timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\n",
    "   \n",
    "    # Momentum Indicators\n",
    "    #....................\n",
    "    data['trend_3']  = MOM(data['Close'], timeperiod=3)\n",
    "    data['trend_5']  = MOM(data['Close'], timeperiod=5)\n",
    "    data['trend_8']  = MOM(data['Close'], timeperiod=8)\n",
    "    data['trend_13'] = MOM(data['Close'], timeperiod=13)\n",
    "    data['trend_34'] = MOM(data['Close'], timeperiod=34)\n",
    "    data['trend_55'] = MOM(data['Close'], timeperiod=55)\n",
    "    data['trend_100'] = MOM(data['Close'], timeperiod=100)\n",
    "    data['trend_200'] = MOM(data['Close'], timeperiod=200)\n",
    "    data['MACD'],data['MACDsignal'],macdhist = MACD(data['Close'], fastperiod=12, slowperiod=26, signalperiod=9)  \n",
    "    data['RSI-14'] = RSI(data['Close'],timeperiod=14)\n",
    "    data['ADX-14'] = ADX(data['High'],data['Low'],data['Close'], timeperiod=14)\n",
    "\n",
    "    # Volume Indicators\n",
    "    #..................\n",
    "    data['MFI-14'] = MFI(data['High'],data['Low'],data['Close'],data['Volume'],timeperiod=14)\n",
    "    data['OBV']    = OBV(data['Close'],data['Volume'])\n",
    "    data['ADOSC']  = ADOSC(data['High'],data['Low'],data['Close'],data['Volume'],fastperiod=3,slowperiod=10)\n",
    "    \n",
    "    \n",
    "    #Daily pivots calculation\n",
    "    #........................\n",
    "    val_init =  data.iloc[0]['Close']\n",
    "    data['Pre_Close'] = data[\"Close\"].shift(periods=1)\n",
    "    data.iloc[0]['Pre_Close'] = val_init\n",
    "\n",
    "    val_init =  data.iloc[0]['High']\n",
    "    data['Pre_High'] = data['High'].shift(periods=1)\n",
    "    data.iloc[0]['Pre_High'] = val_init\n",
    "\n",
    "    val_init =  data.iloc[0]['Low']\n",
    "    data['Pre_Low'] = data['Low'].shift(periods=1)\n",
    "    data.iloc[0]['Pre_Low'] = val_init\n",
    "\n",
    "    data['Pivot'] =  (data['Pre_High']+data['Pre_Low']+data['Pre_Close'])/3\n",
    "    data['R1']    =  (data['Pivot']*2) - data['Pre_Low']\n",
    "    data['R2']    =  data['Pivot'] + data['Pre_High'] - data['Pre_Low']\n",
    "    data['S1']    =  (data['Pivot']*2) - data['Pre_High']\n",
    "    data['S2']    =  data['Pivot'] - data['Pre_High'] + data['Pre_Low']\n",
    "\n",
    "    del data['Pre_High']\n",
    "    del data['Pre_Low']\n",
    "    del data['Pre_Close']\n",
    "    \n",
    "    \n",
    "    # Weekly pivot points calculation\n",
    "    #................................\n",
    "    week_indices = []\n",
    "    week_indices.append(2)\n",
    "\n",
    "    for i in tqdm(range(2,len(data)-1)):\n",
    "        current_day = data.iloc[i]['Day']\n",
    "        next_day    = data.iloc[i+1]['Day']\n",
    "        if (current_day >= next_day):\n",
    "            week_indices.append(i+1)\n",
    "\n",
    "    low_prices   = data['Low']\n",
    "    high_prices  = data['High']\n",
    "    close_prices = data['Close']\n",
    "    weekly_pivots = []\n",
    "    weekly_r1 =[]\n",
    "    weekly_s1 =[]\n",
    "    weekly_r2 =[]\n",
    "    weekly_s2 =[]\n",
    "    \n",
    "    curr_week_index = len(week_indices)-1\n",
    "    while (curr_week_index >= 1):\n",
    "        prev_week = week_indices[curr_week_index-1]\n",
    "        curr_week = week_indices[curr_week_index]\n",
    "        if(curr_week != len(data)-1):\n",
    "            next_week = week_indices[curr_week_index+1]\n",
    "\n",
    "        prev_week_high = max(list(data['High'].iloc[prev_week:curr_week]))\n",
    "        prev_week_low  = min(list(data['Low'].iloc[prev_week:curr_week]))\n",
    "        prev_week_close = data['Close'].iloc[curr_week-1]             \n",
    "    \n",
    "        Pivot_for_curr_week =  (prev_week_high + prev_week_low + prev_week_close)/3\n",
    "        R1_for_curr_week    =  (Pivot_for_curr_week*2) - prev_week_low\n",
    "        R2_for_curr_week    =  Pivot_for_curr_week+prev_week_high - prev_week_low\n",
    "        S1_for_curr_week    =  (Pivot_for_curr_week*2) - prev_week_high\n",
    "        S2_for_curr_week    =  Pivot_for_curr_week - prev_week_high + prev_week_low\n",
    "\n",
    "        if(curr_week != len(data)-1):              \n",
    "            num_days_in_curr_week = next_week - curr_week\n",
    "        else:\n",
    "            num_days_in_curr_week = 1\n",
    "\n",
    "        weekly_pivots = [Pivot_for_curr_week]*(num_days_in_curr_week) + weekly_pivots\n",
    "        weekly_r1     = [R1_for_curr_week]*(num_days_in_curr_week) + weekly_r1 \n",
    "        weekly_r2     = [R2_for_curr_week]*(num_days_in_curr_week) + weekly_r2\n",
    "        weekly_s1     = [S1_for_curr_week]*(num_days_in_curr_week) + weekly_s1\n",
    "        weekly_s2     = [S2_for_curr_week ]*(num_days_in_curr_week)+ weekly_s2\n",
    "\n",
    "        curr_week_index = curr_week_index -1\n",
    "        #End of While Loop\n",
    "    \n",
    "    #Neglect this part of code this is adjusment for initial data (2004 data)                          \n",
    "    len_diff = len(data)-len(weekly_pivots)                            \n",
    "    weekly_pivots = [(data[\"High\"].iloc[0])]*len_diff  + weekly_pivots\n",
    "    weekly_r1     = [(data[\"High\"].iloc[0])]*len_diff  + weekly_r1 \n",
    "    weekly_r2     = [(data[\"High\"].iloc[0])]*len_diff  + weekly_r2\n",
    "    weekly_s1     = [(data[\"High\"].iloc[0])]*len_diff  + weekly_s1 \n",
    "    weekly_s2     = [(data[\"High\"].iloc[0])]*len_diff  + weekly_s2     \n",
    "                         \n",
    "    #print(len(weekly_pivots))\n",
    "    #print(len(data))\n",
    "    data['Weekly_Pivots'] = weekly_pivots\n",
    "    data['Weekly_R1']     = weekly_r1 \n",
    "    data['Weekly_R2']     = weekly_r2\n",
    "    data['Weekly_S1']     = weekly_s1\n",
    "    data['Weekly_S2']     = weekly_s2\n",
    "    \n",
    "    \n",
    "    #MVWAP-7,MVWAP-14,MVWAP-21\n",
    "    #.........................\n",
    "    Mvwap_7  =[]\n",
    "    Mvwap_14 =[]\n",
    "    Mvwap_21 =[]\n",
    "    \n",
    "    data[\"Typical Price\"] = (data['High']+data['Low']+data['Close'])/3\n",
    "    \n",
    "    for i in tqdm(range(6,len(data))):\n",
    "        cumm_TP = ( (data['Typical Price'].iloc[i-6:i+1]) * (data['Volume'].iloc[i-6:i+1]) ).sum()\n",
    "        cumm_vol = data['Volume'].iloc[i-6:i+1].sum()\n",
    "        Mvwap_7.append(cumm_TP/cumm_vol)\n",
    "        \n",
    "    for i in tqdm(range(13,len(data))):\n",
    "        cumm_TP = ( (data['Typical Price'].iloc[i-13:i+1]) * (data['Volume'].iloc[i-13:i+1]) ).sum()\n",
    "        cumm_vol = data['Volume'].iloc[i-13:i+1].sum()\n",
    "        Mvwap_14.append(cumm_TP/cumm_vol)                        \n",
    "        \n",
    "    for i in tqdm(range(20,len(data))):\n",
    "        cumm_TP = ( (data['Typical Price'].iloc[i-20:i+1]) * (data['Volume'].iloc[i-20:i+1]) ).sum()\n",
    "        cumm_vol = data['Volume'].iloc[i-20:i+1].sum()\n",
    "        Mvwap_21.append(cumm_TP/cumm_vol)\n",
    "       \n",
    "        \n",
    "    #Neglect the next lines , These are Just for maintaining the length of the data frame columns \n",
    "    some_val  = (data['Typical Price'].iloc[0] * data['Volume'].iloc[0]) / (data['Volume'].iloc[0])\n",
    "    Mvwap_7   = [some_val]*6 + Mvwap_7 \n",
    "    Mvwap_14  = [some_val]*13 + Mvwap_14\n",
    "    Mvwap_21  = [some_val]*20 + Mvwap_21\n",
    "    \n",
    "    data['MVWAP-7']  = Mvwap_7 \n",
    "    data['MVWAP-14'] = Mvwap_14\n",
    "    data['MVWAP-21'] = Mvwap_14\n",
    "    \n",
    "    \n",
    "    # PRICE ACTION INDICATORS\n",
    "    # ..................\n",
    "    \n",
    "    data['Day_trend']                       =  data.apply(lambda day: ((day['Close']-day['Open'])*100)/ (day['Open']),axis=1 )\n",
    "    \n",
    "    data['Day_trend_strength_up_shadow']    =  data.apply(lambda day: ((day['High']-day['Close']))/(abs(day['Close']-day['Open'])+0.0001)\n",
    "                                                                      if (day['Day_trend'] >= 0)\n",
    "                                                                      else ((day['High']-day['Open']))/(abs(day['Close']-day['Open'])+0.0001)\n",
    "                                                                     ,axis=1)\n",
    "\n",
    "    data['Day_trend_strength_low_shadow']   =  data.apply(lambda day: ((day['Open']-day['Low']))/(abs(day['Close']-day['Open'])+0.0001)\n",
    "                                                                      if (day['Day_trend'] >= 0)\n",
    "                                                                      else ((day['Close']-day['Low']))/(abs(day['Close']-day['Open'])+0.0001)\n",
    "                                                                     ,axis=1)\n",
    "    \n",
    "    data['pre_close'] = data['Close'].shift()\n",
    "    data = data.iloc[1:]\n",
    "    data['gap_up_down_opening'] = data.apply(lambda day: ((day['Open']-day['pre_close'])*100)/ (day['pre_close']),axis=1 )\n",
    "   \n",
    "    data['Pre_vol'] = data['Volume'].shift()\n",
    "    data = data.iloc[1:]\n",
    "    data['Change_in_Volume'] = data.apply(lambda day: ((day['Volume']-day['Pre_vol'])*100)/(day['Pre_vol']),axis=1 )\n",
    "    \n",
    "    cols_2_drop += ['pre_close','Pre_vol']\n",
    "    data = data.drop(columns=cols_2_drop)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty_with_candle_sticks = pd.read_csv('Nifty50_With_Candle_Patterns.csv')\n",
    "nifty_with_candle_sticks = nifty_with_candle_sticks.set_index('Date')\n",
    "del nifty_with_candle_sticks['Turnover']\n",
    "nifty_with_candle_sticks = nifty_with_candle_sticks.rename(columns={'Number of Contracts':'Volume'})\n",
    "\n",
    "nifty_with_candle_sticks = nifty_with_candle_sticks.reset_index()\n",
    "nifty_with_candle_sticks = nifty_with_candle_sticks.rename(columns={'index':'Date'})\n",
    "nifty_with_candle_sticks['Date']  = nifty_with_candle_sticks['Date'].apply(lambda d: datetime.datetime.strptime(d, '%Y-%m-%d'))\n",
    "nifty_with_candle_sticks['Day']   = nifty_with_candle_sticks['Date'].apply(lambda date: date.weekday())\n",
    "nifty_with_candle_sticks['Month'] = nifty_with_candle_sticks['Date'].apply(lambda date: date.month)\n",
    "nifty_with_candle_sticks['Year']  = nifty_with_candle_sticks['Date'].apply(lambda date: date.year)\n",
    "nifty_with_candle_sticks = nifty_with_candle_sticks.set_index('Date')\n",
    "\n",
    "nifty_tech_indicators = add_technical_indicators(nifty_with_candle_sticks.copy())\n",
    "nifty_with_candle_patterns_n_tech_indicators = pd.concat([nifty_with_candle_sticks,nifty_tech_indicators],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open Interest</th>\n",
       "      <th>Change in OI</th>\n",
       "      <th>CDL2CROWS</th>\n",
       "      <th>CDL3INSIDE</th>\n",
       "      <th>CDL3LINESTRIKE</th>\n",
       "      <th>...</th>\n",
       "      <th>Weekly_S2</th>\n",
       "      <th>Typical Price</th>\n",
       "      <th>MVWAP-7</th>\n",
       "      <th>MVWAP-14</th>\n",
       "      <th>MVWAP-21</th>\n",
       "      <th>Day_trend</th>\n",
       "      <th>Day_trend_strength_up_shadow</th>\n",
       "      <th>Day_trend_strength_low_shadow</th>\n",
       "      <th>gap_up_down_opening</th>\n",
       "      <th>Change_in_Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-01-01</th>\n",
       "      <td>1894.0</td>\n",
       "      <td>1928.00</td>\n",
       "      <td>1890.1</td>\n",
       "      <td>1925.30</td>\n",
       "      <td>74471</td>\n",
       "      <td>7422600</td>\n",
       "      <td>128600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-02</th>\n",
       "      <td>1929.5</td>\n",
       "      <td>1961.00</td>\n",
       "      <td>1929.5</td>\n",
       "      <td>1950.80</td>\n",
       "      <td>73078</td>\n",
       "      <td>7770000</td>\n",
       "      <td>347400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-05</th>\n",
       "      <td>1962.8</td>\n",
       "      <td>1969.75</td>\n",
       "      <td>1931.0</td>\n",
       "      <td>1957.15</td>\n",
       "      <td>101796</td>\n",
       "      <td>8339000</td>\n",
       "      <td>569000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1928.0</td>\n",
       "      <td>1952.633333</td>\n",
       "      <td>1914.466667</td>\n",
       "      <td>1914.466667</td>\n",
       "      <td>1914.466667</td>\n",
       "      <td>-0.287854</td>\n",
       "      <td>1.230067</td>\n",
       "      <td>4.628237</td>\n",
       "      <td>0.615132</td>\n",
       "      <td>39.297737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open     High     Low    Close  Volume  Open Interest  \\\n",
       "Date                                                                  \n",
       "2004-01-01  1894.0  1928.00  1890.1  1925.30   74471        7422600   \n",
       "2004-01-02  1929.5  1961.00  1929.5  1950.80   73078        7770000   \n",
       "2004-01-05  1962.8  1969.75  1931.0  1957.15  101796        8339000   \n",
       "\n",
       "            Change in OI  CDL2CROWS  CDL3INSIDE  CDL3LINESTRIKE  ...  \\\n",
       "Date                                                             ...   \n",
       "2004-01-01        128600        0.0         0.0             0.0  ...   \n",
       "2004-01-02        347400        0.0         0.0             0.0  ...   \n",
       "2004-01-05        569000        0.0         0.0             0.0  ...   \n",
       "\n",
       "            Weekly_S2  Typical Price      MVWAP-7     MVWAP-14     MVWAP-21  \\\n",
       "Date                                                                          \n",
       "2004-01-01        NaN            NaN          NaN          NaN          NaN   \n",
       "2004-01-02        NaN            NaN          NaN          NaN          NaN   \n",
       "2004-01-05     1928.0    1952.633333  1914.466667  1914.466667  1914.466667   \n",
       "\n",
       "            Day_trend  Day_trend_strength_up_shadow  \\\n",
       "Date                                                  \n",
       "2004-01-01        NaN                           NaN   \n",
       "2004-01-02        NaN                           NaN   \n",
       "2004-01-05  -0.287854                      1.230067   \n",
       "\n",
       "            Day_trend_strength_low_shadow  gap_up_down_opening  \\\n",
       "Date                                                             \n",
       "2004-01-01                            NaN                  NaN   \n",
       "2004-01-02                            NaN                  NaN   \n",
       "2004-01-05                       4.628237             0.615132   \n",
       "\n",
       "            Change_in_Volume  \n",
       "Date                          \n",
       "2004-01-01               NaN  \n",
       "2004-01-02               NaN  \n",
       "2004-01-05         39.297737  \n",
       "\n",
       "[3 rows x 107 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nifty_with_candle_patterns_n_tech_indicators.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty_with_candle_patterns_n_tech_indicators.to_csv('Nifty50_With_Candle_Patterns_N_Tech_Indicators.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import yfinance as yf\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REF: https://medium.com/datadriveninvestor/python-utility-for-scrapping-historical-fii-data-ml-data-mining-a804a885df15\n",
    "\n",
    "from selenium.webdriver.chrome import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def Scrap_FII_Data(start_date,end_date):\n",
    "    \"\"\"\n",
    "    This function scraps data(FII's data) from way2wealth.com, \n",
    "    Features are fomed using the FII's Data.Later this data is saved in csv file.\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = ['Date','FII_Futures_Net','FII_Options_Net']\n",
    "    fii_data_df = pd.DataFrame(columns=cols)\n",
    "    browser_options = webdriver.Options()\n",
    "    browser_options.add_argument(\"--start-maximized\")\n",
    "    browser_driver = webdriver.WebDriver(\"chromedriver.exe\",options=browser_options)\n",
    "    browser_driver.get(\"https://www.way2wealth.com/derivatives/fiiactivity/\");\n",
    "    time.sleep(10);\n",
    "    start_date_field = browser_driver.find_element_by_id(\"ContentPlaceHolder1_ctl00_txtstart\")\n",
    "    end_date_field   = browser_driver.find_element_by_id(\"ContentPlaceHolder1_ctl00_txtend\")\n",
    "    start_date_field.clear();\n",
    "    end_date_field.clear(); \n",
    "\n",
    "    start_date_field.send_keys(start_date);\n",
    "    end_date_field.send_keys(end_date);\n",
    "    \n",
    "    go_button = browser_driver.find_element_by_id(\"ContentPlaceHolder1_ctl00_btngo\")\n",
    "    browser_driver.execute_script(\"arguments[0].click();\", go_button)\n",
    "    \n",
    "    # Setting up the xpath for the next-pagination of the table\n",
    "    pagination = 2\n",
    "    xpath_before_pagination = \"//*[@id='ContentPlaceHolder1_ctl00_grdboard']/tbody/tr[19]/td/table/tbody/tr/td[\";\n",
    "    xpath_after_pagination  = \"]/a\";\n",
    "    next_pagination_xpath = xpath_before_pagination+str(pagination)+xpath_after_pagination;\n",
    "    \n",
    "    time.sleep(10);\n",
    "    element_present=True\n",
    "    p=1\n",
    "    while element_present: \n",
    "    # Getting the values(Date, FII net futures and options) for each pagination/page of the table\n",
    "    \n",
    "        date_element_present=True\n",
    "        beforeXpath= \"//*[@id='ContentPlaceHolder1_ctl00_grdboard']/tbody/tr[\";\n",
    "        afterXpath_Date=\"]/td[1]\";\n",
    "        afterXpath_IndexNet=\"]/td[5]\";\n",
    "        afterXpath_IndexOptionNet=\"]/td[13]\";\n",
    "            \n",
    "        for row in range (1,19):\n",
    "            # Getting the values(Date, FII net futures and options) for each row in a page of the table\n",
    "            contentXpath = beforeXpath+str(row)+afterXpath_IndexNet;\n",
    "            try:\n",
    "                IndexNet = browser_driver.find_element_by_xpath(contentXpath);\n",
    "                date_element_present=True\n",
    "            except NoSuchElementException:\n",
    "                date_element_present=False\n",
    "                \n",
    "            if date_element_present:\n",
    "                contentXpath = beforeXpath+str(row)+afterXpath_Date;                    \n",
    "                Date = browser_driver.find_element_by_xpath(contentXpath).text;\n",
    "                    \n",
    "                contentXpath = beforeXpath+str(row)+afterXpath_IndexNet;\n",
    "                IndexNet     = browser_driver.find_element_by_xpath(contentXpath).text;\n",
    "                    \n",
    "                contentXpath   = beforeXpath+str(row)+afterXpath_IndexOptionNet;\n",
    "                IndexOptionNet = browser_driver.find_element_by_xpath(contentXpath).text;\n",
    "                    \n",
    "                fii_data_df =  fii_data_df.append({'Date': Date, \n",
    "                                                       'FII_Futures_Net' : IndexNet,\n",
    "                                                       'FII_Options_Net': IndexOptionNet}, \n",
    "                                                        ignore_index=True)\n",
    "        #print(\"Page {0} done , Next-Pagination {1}\".format(p,pagination))\n",
    "        p=p+1\n",
    "                    \n",
    "        try:\n",
    "            element_present=True   \n",
    "            next_pagination = browser_driver.find_element_by_xpath(next_pagination_xpath);\n",
    "            pagination = pagination+1;\n",
    "            next_pagination_xpath = xpath_before_pagination+str(pagination)+xpath_after_pagination;\n",
    "            \n",
    "        except NoSuchElementException:\n",
    "                print(next_pagination_xpath)\n",
    "                element_present=False\n",
    "                \n",
    "        if element_present:\n",
    "            browser_driver.execute_script(\"arguments[0].click();\",next_pagination);\n",
    "            time.sleep(10);\n",
    "        else:\n",
    "            break;\n",
    "            \n",
    "    browser_driver.quit();\n",
    "    return fii_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date,timedelta\n",
    "\n",
    "time_periods = []\n",
    "start_date = date(2009,1,31)\n",
    "end_date = date(2001,1,1)\n",
    "\n",
    "while (end_date<date(2020,12,8)):\n",
    "    end_date   = start_date + timedelta(days=180)\n",
    "    s_d   = start_date.strftime('%m/%d/%Y')\n",
    "    e_d   = end_date.strftime('%m/%d/%Y')\n",
    "    entry = (s_d,e_d)\n",
    "    time_periods.append(entry)\n",
    "    start_date = end_date + timedelta(days=1)\n",
    "    \n",
    "period_count = 1\n",
    "for period in tqdm(time_periods):\n",
    "    fii_data = Scrap_FII_Data(period[0],period[1])\n",
    "    fii_data.to_csv('fii_'+str(period_count)+'.csv')\n",
    "    period_count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = ('./Source Data Sets/Fii/fii_1.csv')\n",
    "fii_final_data = pd.read_csv(file_name)\n",
    "fii_final_data = fii_final_data.iloc[::-1]\n",
    "del fii_final_data['Unnamed: 0']\n",
    "    \n",
    "for i in range(2,25):\n",
    "    file_name = ('./Source Data Sets/Fii/fii_'+str(i)+'.csv')\n",
    "    curr_fii_data = pd.read_csv(file_name)\n",
    "    curr_fii_data = curr_fii_data.iloc[::-1]\n",
    "    del curr_fii_data['Unnamed: 0']\n",
    "    fii_final_data = pd.concat([fii_final_data,curr_fii_data],axis=0)\n",
    "    \n",
    "fii_final_data = fii_final_data.set_index('Date')\n",
    "fii_final_data['pre_fut_net'] = fii_final_data['FII_Futures_Net'].shift()\n",
    "fii_final_data['pre_opt_net'] = fii_final_data['FII_Options_Net'].shift()\n",
    "fii_final_data = fii_final_data.iloc[1:]\n",
    "\n",
    "fii_final_data['FII_Futures_Net_change'] = fii_final_data['FII_Futures_Net'] - fii_final_data['pre_fut_net'] \n",
    "fii_final_data['FII_Options_Net_change'] = fii_final_data['FII_Options_Net'] - fii_final_data['pre_opt_net'] \n",
    "\n",
    "del fii_final_data['pre_fut_net']\n",
    "del fii_final_data['pre_opt_net']\n",
    "\n",
    "#print(fii_final_data)\n",
    "fii_final_data.to_csv('Final_FII_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_world_stock_indices_data(index,start_date,end_date):\n",
    "    \"\"\"\n",
    "    This Function gets the world stock indices historical day wise data by making use of yfinance library.\n",
    "    From Each country's index data, features are formed.\n",
    "    Each country's features data is saved in seperate csv files.\n",
    "    \n",
    "    \"\"\"\n",
    "    if   (index=='^DJI'):\n",
    "        country = 'US'\n",
    "    elif (index=='^FTSE'):\n",
    "        country = 'Lon'\n",
    "    elif (index=='^FCHI'):\n",
    "        country = 'Fran'\n",
    "    elif (index=='000001.SS'):\n",
    "        country = 'China'\n",
    "    elif (index=='^N225'):\n",
    "        country = 'Japan'\n",
    "    elif (index=='^STI'):\n",
    "        country = 'Singp'\n",
    "    elif (index=='^HSI'):\n",
    "        country = 'Honkon'\n",
    "        \n",
    "        \n",
    "    data = yf.download(index, start=start_date, end=end_date)\n",
    "    print(country)\n",
    "    print(data[['Open','High','Low','Close']].head(5))\n",
    "    print(\".\"*60)\n",
    "    \n",
    "    country_Openchange_from_pre_clo   = country+'_gap_up_down_opening'\n",
    "    country_day_trend                 = country+'_day_trend'\n",
    "    country_trend_strength_up_shadow  = country+'_trend_strength_up_shadow'\n",
    "    country_trend_strength_low_shadow = country+'_trend_strength_low_shadow'\n",
    "    \n",
    "    \n",
    "    data[country_day_trend]                   =  data.apply(lambda day: ((day['Close']-day['Open'])*100)/ (day['Open']),axis=1 )\n",
    "    \n",
    "    data[country_trend_strength_up_shadow]    =  data.apply(lambda day: ((day['High']-day['Close']))/(abs(day['Close']-day['Open'])+0.0001)\n",
    "                                                         if (day[country_day_trend] >= 0)\n",
    "                                                         else ((day['High']-day['Open']))/(abs(day['Close']-day['Open'])+0.0001)\n",
    "                                                         ,axis=1)\n",
    "    \n",
    "    data[country_trend_strength_low_shadow]   =  data.apply(lambda day: ((day['Open']-day['Low']))/(abs(day['Close']-day['Open'])+0.0001)\n",
    "                                                         if (day[country_day_trend] >= 0)\n",
    "                                                         else ((day['Close']-day['Low']))/(abs(day['Close']-day['Open'])+0.0001)\n",
    "                                                         ,axis=1)\n",
    "    \n",
    "    \n",
    "    if (index == '^FTSE'):\n",
    "        cols_2_drop = ['Open','High','Low','Close','Adj Close','Volume']\n",
    "    else:\n",
    "        data['pre_close'] = data['Close'].shift()\n",
    "        data = data.iloc[1:]\n",
    "        data[country_Openchange_from_pre_clo]   = data.apply(lambda day: ((day['Open']-day['pre_close'])*100)/ (day['pre_close']),axis=1 )\n",
    "\n",
    "        cols_2_drop = ['Open','High','Low','Close','Adj Close','Volume','pre_close']\n",
    "        \n",
    "    data = data.drop(columns=cols_2_drop)\n",
    "    return data\n",
    "\n",
    "\n",
    "#..............................................................................\n",
    "\n",
    "us_dow_index_data = get_world_stock_indices_data('^DJI',\"2009-1-30\",\"2020-12-08\")\n",
    "us_dow_index_data.to_csv('us_index_data.csv')\n",
    "\n",
    "lon_index_data = get_world_stock_indices_data('^FTSE',\"2009-1-30\",\"2020-12-08\")\n",
    "lon_index_data.to_csv('london_index_data.csv')\n",
    "\n",
    "fran_index_data = get_world_stock_indices_data('^FCHI',\"2009-1-30\",\"2020-12-08\")\n",
    "fran_index_data.to_csv('france_index_data.csv')\n",
    "\n",
    "chi_index_data = get_world_stock_indices_data('000001.SS',\"2009-1-30\",\"2020-12-08\")\n",
    "chi_index_data.to_csv('china_index_data.csv')\n",
    "\n",
    "jap_index_data = get_world_stock_indices_data('^N225',\"2009-1-30\",\"2020-12-08\")\n",
    "jap_index_data.to_csv('japan_index_data.csv')\n",
    "\n",
    "honkon_index_data = get_world_stock_indices_data('^HSI',\"2009-1-30\",\"2020-12-08\")\n",
    "honkon_index_data.to_csv('honkong_index_data.csv')\n",
    "\n",
    "singa_index_data = get_world_stock_indices_data('^STI',\"2009-1-30\",\"2020-12-08\")\n",
    "singa_index_data.to_csv('singapore_index_data.csv')\n",
    "\n",
    "\n",
    "\n",
    "# 000001.SS     - china      (7.00am  to 12.30pm)    IST         2hrs        ---- feb 2nd 2009   -2h candle  (UTC+8) Shangai\n",
    "# ^HSI          - honkon     (6.45am  to 1.30am)     IST         2hrs 15mins ---- feb 2nd 2009   -3h candle\n",
    "# ^N225         - japan      (5.30am  to 11.30am)    IST         3hrs 30mins ---- feb 2nd 2009   -4h candle\n",
    "# ^NSEI         - India\n",
    "\n",
    "# ^DJI          - US         (7.00pm  to 1.30am)     IST      \n",
    "# ^FTSE         - London     (1.30pm  to  10pm)      IST\n",
    "# ^FCHI         - CAC 40     (12.30pm to 10pm)       IST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Todays_World_Indices_Data_Till_9AM(Country,c_data):\n",
    "    \"\"\"\n",
    "    This Function gets the Asian markets indices historical day wise data till 9am(IST).\n",
    "    From Each country's index data, features are formed.\n",
    "    Each country's features data is saved in seperate csv files.\n",
    "    \"\"\"\n",
    "    data = c_data\n",
    "    country = Country\n",
    "    add_utc_hours = 8\n",
    "    \n",
    "    if  (country == 'china' or country == 'hongkong'):\n",
    "        add_utc_hours = 8\n",
    "    elif (country == 'japan'):\n",
    "        add_utc_hours = 9\n",
    "  \n",
    "        \n",
    "    data['Date_Time'] = data['time'].apply(lambda date_unix:( (datetime.datetime.fromtimestamp(date_unix))\n",
    "                                                              .strftime('%Y-%m-%d %H:%M'))                 \n",
    "                                           )\n",
    "\n",
    "    data[['Date','Time']] = data['Date_Time'].str.split(' ', 1, expand=True)\n",
    "    data = data[(data['Time'].str)[0:2]=='09']\n",
    "\n",
    "    \n",
    "    country_day_trend                 = country+'_today_init_trend'\n",
    "    country_trend_strength_up_shadow  = country+'_today_init_trend_strength_up_shadow'\n",
    "    country_trend_strength_low_shadow = country+'_today_init_trend_strength_low_shadow'\n",
    "    \n",
    "    data[country_day_trend]                   =  data.apply(lambda day: ((day['close']-day['open'])*100)/ (day['open']),axis=1 )\n",
    "    data[country_trend_strength_up_shadow]    =  data.apply(lambda day: ((day['high']-day['close']))/(abs(day['close']-day['open'])+0.0001)\n",
    "                                                             if (day[country_day_trend] >= 0)\n",
    "                                                             else ((day['high']-day['open']))/(abs(day['close']-day['open'])+0.0001)\n",
    "                                                             ,axis=1)\n",
    "\n",
    "    data[country_trend_strength_low_shadow]   =  data.apply(lambda day: ((day['open']-day['low']))/(abs(day['close']-day['open'])+0.0001)\n",
    "                                                             if (day[country_day_trend] >= 0)\n",
    "                                                             else ((day['close']-day['low']))/(abs(day['close']-day['open'])+0.0001)\n",
    "                                                             ,axis=1)\n",
    "    \n",
    "    cols_2_drop =['open','close','high','low','Volume','Volume MA','Date_Time','Time','time']\n",
    "    data = data.drop(columns=cols_2_drop)\n",
    "    data = data.set_index('Date')\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "china_data_df = pd.read_csv('SSE_DLY_000001, 120.csv')\n",
    "china_data    = get_Todays_World_Indices_Data_Till_9AM('china',china_data_df)\n",
    "china_data.to_csv('china_todays_init_data.csv')\n",
    "\n",
    "hon_kon_data_df = pd.read_csv('HSI_HSI, 180.csv')\n",
    "hon_kon_data    = get_Todays_World_Indices_Data_Till_9AM('hongkong',hon_kon_data_df)\n",
    "hon_kon_data.to_csv('hongkong_todays_init_data.csv')\n",
    "\n",
    "japan_data_df = pd.read_csv('TVC_NI225, 240.csv')\n",
    "japan_data    = get_Todays_World_Indices_Data_Till_9AM('japan',japan_data_df)\n",
    "japan_data.to_csv('japan_todays_init_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brent_crude_prices_data():\n",
    "    \"\"\"\n",
    "    This Function reads crude oil price's historical data and forms features from it. \n",
    "    crude oil features are saved in a csv file.\n",
    "    \"\"\"\n",
    "    crude_oil = pd.read_csv('BrentCrude.csv')\n",
    "    crude_oil['Date'] = crude_oil['Date'].apply(lambda date: (parser.parse(date)).strftime('%d-%m-%Y')) \n",
    "    end_index = crude_oil[ crude_oil['Date']=='30-01-2009'].index[0]\n",
    "    crude_oil = crude_oil[:end_index+1]\n",
    "    crude_oil = crude_oil.iloc[::-1]\n",
    "    crude_oil['pre_price'] = crude_oil['Brent Crude Price'].shift()\n",
    "    crude_oil['change_in_crude_price'] = crude_oil['Brent Crude Price']-crude_oil['pre_price']\n",
    "    crude_oil = crude_oil.set_index('Date')\n",
    "    del crude_oil['pre_price']\n",
    "    del crude_oil['Brent Crude Price']\n",
    "    crude_oil = crude_oil[1:]\n",
    "    #crude_oil.head()\n",
    "    #crude_oil .tail()\n",
    "\n",
    "\n",
    "    crude_oil_2 = pd.read_csv('HistoricalQuotes.csv')\n",
    "    crude_oil_2['Date'] = crude_oil_2['Date'].apply(lambda date: (parser.parse(date)).strftime('%d-%m-%Y')) \n",
    "    end_index = crude_oil_2[ crude_oil_2['Date']=='21-06-2019'].index[0]\n",
    "    crude_oil_2 = crude_oil_2[:end_index+1]\n",
    "    crude_oil_2= crude_oil_2.iloc[::-1]\n",
    "    crude_oil_2 = crude_oil_2.rename(columns={' Close':'Brent Crude Price'})\n",
    "    crude_oil_2['pre_price'] = crude_oil_2['Brent Crude Price'].shift()\n",
    "    crude_oil_2['change_in_crude_price'] = crude_oil_2['Brent Crude Price']-crude_oil_2['pre_price']\n",
    "    crude_oil_2 = crude_oil_2.set_index('Date')\n",
    "    del crude_oil_2['pre_price']\n",
    "    del crude_oil_2['Brent Crude Price']\n",
    "    crude_oil_2 = crude_oil_2[1:]\n",
    "    \n",
    "    crude_oil_df = pd.concat([crude_oil,crude_oil_2],axis=0)\n",
    "    #crude_oil_2.head(5)\n",
    "    #crude_oil .tail()\n",
    "    return crude_oil_df\n",
    "\n",
    "    \n",
    "crude_oil_df = get_brent_crude_prices_data()\n",
    "crude_oil_df.to_csv(\"Brent_crude_prices_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usd_inr_change():\n",
    "    \"\"\"\n",
    "    This Function gets the USD_INR historical data by making use of yfinance library.\n",
    "    USD_INR features are formed from the obtained data and are saved in a csv file.\n",
    "    \"\"\"\n",
    "    data = yf.download(\"INR=X\", start=\"2009-1-30\",end=\"2020-12-8\",prepost = True)\n",
    "    \n",
    "    cols_2_drop  = ['Open','High','Low','Adj Close','Volume']\n",
    "    data = data.drop(columns=cols_2_drop)\n",
    "\n",
    "    data['pre_close'] = data['Close'].shift()\n",
    "    data['Change_in_dollar_rate'] = data['Close']-data['pre_close']\n",
    "    data = data.iloc[2:]\n",
    "    del data['pre_close']\n",
    "    del data['Close']\n",
    "\n",
    "    return data\n",
    "\n",
    "usd_inr_df = get_usd_inr_change()\n",
    "usd_inr_df.to_csv(\"USD_INR_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nsepy import get_history\n",
    "from datetime import date\n",
    "\n",
    "def get_india_vix():\n",
    "    \"\"\"\n",
    "    This Function reads the india_vix historical data.\n",
    "    india_vix features are formed from the obtained data and are saved in a csv file.\n",
    "    \"\"\"\n",
    "    \n",
    "    vix_data_1 = pd.read_csv('./Source Data Sets/Others/India VIX Historical Data.csv')\n",
    "    \n",
    "    end_index = vix_data_1[ vix_data_1['Date'] == '30-01-2009'].index[0]\n",
    "    vix_data_1 = vix_data_1.iloc[:end_index+1]\n",
    "    \n",
    "    vix_data_1 = vix_data_1.iloc[::-1]\n",
    "    vix_data_1  = vix_data_1.set_index('Date')\n",
    "    \n",
    "    vix_data_2 =get_history(symbol=\"INDIAVIX\",\n",
    "                start=date(2019,6,24),\n",
    "                end=date(2020,12,8),\n",
    "                index=True)\n",
    "\n",
    "    vix_data_2 = vix_data_2.dropna()\n",
    "    vix_data_2 = vix_data_2[['Close','Previous']]\n",
    "    del vix_data_2['Previous']\n",
    "    \n",
    "    vix_data_2 = vix_data_2.rename(columns={'Close':'VIX'})\n",
    "    VIX_DATA = pd.concat([vix_data_1,vix_data_2],axis=0)\n",
    "    \n",
    "    VIX_DATA['pre_VIX'] = VIX_DATA['VIX'].shift()\n",
    "    VIX_DATA  = VIX_DATA.iloc[1:]\n",
    "    VIX_DATA['VIX'] = ( (VIX_DATA['VIX']-VIX_DATA['pre_VIX']) / (VIX_DATA['pre_VIX']) ) *100\n",
    "    del VIX_DATA['pre_VIX']\n",
    "      \n",
    "    return VIX_DATA\n",
    "    \n",
    "\n",
    "india_vix_df = get_india_vix()\n",
    "india_vix_df.to_csv('India_VIX_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_order = [\n",
    "             'Open','High','Low','Close','Typical Price','Day_trend','Day_trend_strength_up_shadow','Day_trend_strength_low_shadow',\n",
    "             'gap_up_down_opening','Volume','Change_in_Volume','Open Interest','Change in OI',\n",
    "             'Pivot','R1','R2','S1','S2',\n",
    "             'Weekly_Pivots','Weekly_R1','Weekly_R2','Weekly_S1','Weekly_S2',\n",
    "             'MVWAP-7','MVWAP-14','MVWAP-21',\n",
    "             'trend_3','trend_5','trend_8','trend_13','trend_34','trend_55','trend_100','trend_200',\n",
    "             'EMA_3','EMA_5','EMA_8','EMA_13','EMA_21','EMA_34','EMA_55','EMA_100','EMA_200',\n",
    "             'BollUp','BollMid','BollDown','MACD','MACDsignal','RSI-14','ADX-14','MFI-14','OBV','ADOSC','Day','Month',\n",
    "             'CDL2CROWS','CDL3INSIDE','CDL3LINESTRIKE','CDL3OUTSIDE','CDLADVANCEBLOCK','CDLBELTHOLD','CDLBREAKAWAY',\n",
    "             'CDLCLOSINGMARUBOZU','CDLCOUNTERATTACK','CDLDARKCLOUDCOVER','CDLDOJI','CDLDOJISTAR','CDLDRAGONFLYDOJI',\n",
    "             'CDLENGULFING','CDLEVENINGDOJISTAR','CDLEVENINGSTAR','CDLGAPSIDESIDEWHITE','CDLGRAVESTONEDOJI',\n",
    "             'CDLHAMMER','CDLHANGINGMAN','CDLHARAMI','CDLHARAMICROSS','CDLHIGHWAVE','CDLHIKKAKE','CDLHIKKAKEMOD',\n",
    "             'CDLHOMINGPIGEON','CDLINNECK','CDLINVERTEDHAMMER','CDLLADDERBOTTOM','CDLLONGLEGGEDDOJI','CDLLONGLINE',\n",
    "             'CDLMARUBOZU','CDLMATCHINGLOW','CDLMORNINGDOJISTAR','CDLMORNINGSTAR','CDLONNECK','CDLPIERCING',\n",
    "             'CDLRICKSHAWMAN','CDLSEPARATINGLINES','CDLSHOOTINGSTAR','CDLSHORTLINE','CDLSPINNINGTOP','CDLSTALLEDPATTERN',\n",
    "             'CDLSTICKSANDWICH','CDLTAKURI','CDLTASUKIGAP','CDLTHRUSTING','CDLTRISTAR','CDLUNIQUE3RIVER',\n",
    "             'CDLUPSIDEGAP2CROWS','CDLXSIDEGAP3METHODS'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[cols_order]\n",
    "data.to_csv('Nifty_Technical_Analysis_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2895, 106)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_df = pd.read_csv('Nifty_Technical_Analysis_Data.csv')\n",
    "Final_df = Final_df.set_index('Date')\n",
    "Final_df .shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <font color ='red'>JOIINING ALL THE DATA PRESENT ACROSS MULTIPLE CSV FILES<b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# Files = os.listdir()\n",
    "\n",
    "Files = [ \n",
    "          'Final_FII_Data.csv','India_VIX_data.csv', 'USD_INR_data.csv','Brent_crude_prices_data.csv',\n",
    "          'us_index_data.csv','london_index_data.csv', 'france_index_data.csv','singapore_index_data.csv',\n",
    "          'china_index_data.csv','china_todays_init_data.csv',\n",
    "          'honkong_index_data.csv', 'hongkong_todays_init_data.csv',\n",
    "          'japan_index_data.csv', 'japan_todays_init_data.csv',\n",
    "        ]\n",
    "\n",
    "for file in Files:\n",
    "    data = pd.read_csv(file)\n",
    "    #print(str( len(data['Date']) ) + ' : ' + str( len(data['Date'].unique()) ) ) \n",
    "    data = data.set_index('Date')\n",
    "    Final_df = Final_df.join(data)\n",
    "    \n",
    "Final_df.to_csv('Final_Source_Data.csv')\n",
    "print(Final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_df = Final_df.fillna(method='ffill',axis=0)\n",
    "Final_df.to_csv('Final_Source_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_df = pd.read_csv('Final_Source_Data.csv')\n",
    "Final_df = Final_df.set_index('Date')\n",
    "#print(len((Final_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_rename_dict = {\n",
    "'Open' : 'Open_T' , 'High'  : 'High_T' ,'Low'  : 'Low_T' , 'Close' : 'Close_T' ,'Day_trend' : 'Day_trend_T' ,'gap_up_down_opening' : 'gap_up_down_opening_T', \n",
    "'Pivot' : 'Pivot_T','R1': 'R1_T','R2': 'R2_T','S1':'S1_T','S2':'S2_T','Weekly_Pivots':'Weekly_Pivots_T','Weekly_R1':'Weekly_R1_T','Weekly_R2':'Weekly_R2_T',\n",
    "'Weekly_S1':'Weekly_S1_T','Weekly_S2':'Weekly_S2_T','RSI-14' : 'RSI-14_T' ,'MFI-14' : 'MFI-14_T','Day' : 'Day_T','Month': 'Month_T',\n",
    "'Singp_gap_up_down_opening':'Singp_gap_up_down_opening_T','China_gap_up_down_opening':'China_gap_up_down_opening_T',\n",
    "'china_today_init_trend':'china_today_init_trend_T','china_today_init_trend_strength_up_shadow':'china_today_init_trend_strength_up_shadow_T',\n",
    "'china_today_init_trend_strength_low_shadow' : 'china_today_init_trend_strength_low_shadow_T' ,'Honkon_gap_up_down_opening' : 'Honkon_gap_up_down_opening_T',\n",
    "'hongkong_today_init_trend': 'hongkong_today_init_trend_T','hongkong_today_init_trend_strength_up_shadow' : 'hongkong_today_init_trend_strength_up_shadow_T',\n",
    "'hongkong_today_init_trend_strength_low_shadow': 'hongkong_today_init_trend_strength_low_shadow_T','Japan_gap_up_down_opening' : 'Japan_gap_up_down_opening_T', \n",
    "'japan_today_init_trend' : 'japan_today_init_trend_T',\n",
    "'japan_today_init_trend_strength_up_shadow' : 'japan_today_init_trend_strength_up_shadow_T', \n",
    "'japan_today_init_trend_strength_low_shadow' :  'japan_today_init_trend_strength_low_shadow_T'\n",
    "}\n",
    "\n",
    "\n",
    "shift_helper_data_df  = Final_df\n",
    "\n",
    "Final_df_copy = Final_df.copy()\n",
    "Final_df_copy = Final_df_copy.rename(columns = cols_rename_dict)\n",
    "Today_df = Final_df_copy[ (list(cols_rename_dict.values())) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2893, 149)\n",
      "(2893, 33)\n"
     ]
    }
   ],
   "source": [
    "print(Final_df.shape)\n",
    "print(Today_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='red'>COMBINING PREVIOUS 5 DAYS DATA.</font>\n",
    "\n",
    "Current Day's Input data point will contain...<br><br>\n",
    "    \n",
    "\n",
    "T  day Features(Current day's features)<br>\n",
    "T1 day Features(Previous day's features) <br>\n",
    "T2 day Features <br>\n",
    "T3 day Features<br>\n",
    "T4 day Features<br>\n",
    "T5 day Features<br>\n",
    "   \n",
    "<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  combine_last_5_days(data,shift_data):\n",
    "    \"\"\"\n",
    "    This function joins the previous 5 days data for each current day data point.\n",
    "    \n",
    "    \"\"\"\n",
    "    final_data = data.copy()\n",
    "    shift_data = shift_data.shift(1)\n",
    "    final_data = final_data.join(shift_data)\n",
    "    \n",
    "    for i in range(2,6):\n",
    "        suffix_l = '_T'+str(i-1)\n",
    "        suffix_r = '_T'+str(i)\n",
    "        shift_data = shift_data.shift(1)\n",
    "        \n",
    "        if(i==5):\n",
    "            final_data = final_data.join(shift_data,lsuffix=suffix_l,rsuffix=suffix_r)\n",
    "        else:\n",
    "            final_data = final_data.join(shift_data,lsuffix=suffix_l)\n",
    "            \n",
    "    return final_data\n",
    "\n",
    "FINAL_DATA_DF = combine_last_5_days(Today_df,shift_helper_data_df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='red'>For each Day's data point,Forming the Y's data (i.e output or the classes data) and saving the FINAL DATA FILE.CSV</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_DATA_DF = FINAL_DATA_DF.dropna()\n",
    "FINAL_DATA_DF['Y_change'] = FINAL_DATA_DF['Close_T']-FINAL_DATA_DF['Open_T']\n",
    "# FINAL_DATA_DF['Y_trend']  = FINAL_DATA_DF.apply(lambda day: 0 if (day['Y_change'] < 0) else 1, axis =1)\n",
    "# FINAL_DATA_DF['Y_high']   = FINAL_DATA_DF['High_T']\n",
    "# FINAL_DATA_DF['Y_low']    = FINAL_DATA_DF['Low_T']\n",
    "\n",
    "cols_2_drop = ['Close_T','High_T','Low_T','Day_trend_T']\n",
    "\n",
    "FINAL_DATA_DF = FINAL_DATA_DF.drop(columns=cols_2_drop)\n",
    "FINAL_DATA_DF.to_csv('FINAL_DATA.csv')\n",
    "FINAL_DATA_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('FINAL_DATA - changes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2888, 599)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
